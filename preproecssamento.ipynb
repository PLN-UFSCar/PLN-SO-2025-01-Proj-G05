{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9e999e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/penido/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mrslp\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('rslp')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mstemmers/rslp/step0.pt\u001b[0m\n\n  Searched in:\n    - '/home/penido/nltk_data'\n    - '/home/penido/miniconda3/envs/pln/nltk_data'\n    - '/home/penido/miniconda3/envs/pln/share/nltk_data'\n    - '/home/penido/miniconda3/envs/pln/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m nltk.download(\u001b[33m'\u001b[39m\u001b[33mstopwords\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     12\u001b[39m stopwords_pt = \u001b[38;5;28mset\u001b[39m(stopwords.words(\u001b[33m'\u001b[39m\u001b[33mportuguese\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m stemmer_pt = \u001b[43mRSLPStemmer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m nlp = spacy.load(\u001b[33m'\u001b[39m\u001b[33mpt_core_news_sm\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pln/lib/python3.12/site-packages/nltk/stem/rslp.py:56\u001b[39m, in \u001b[36mRSLPStemmer.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     54\u001b[39m     \u001b[38;5;28mself\u001b[39m._model = []\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     \u001b[38;5;28mself\u001b[39m._model.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread_rule\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstep0.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     57\u001b[39m     \u001b[38;5;28mself\u001b[39m._model.append(\u001b[38;5;28mself\u001b[39m.read_rule(\u001b[33m\"\u001b[39m\u001b[33mstep1.pt\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     58\u001b[39m     \u001b[38;5;28mself\u001b[39m._model.append(\u001b[38;5;28mself\u001b[39m.read_rule(\u001b[33m\"\u001b[39m\u001b[33mstep2.pt\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pln/lib/python3.12/site-packages/nltk/stem/rslp.py:65\u001b[39m, in \u001b[36mRSLPStemmer.read_rule\u001b[39m\u001b[34m(self, filename)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_rule\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     rules = \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnltk:stemmers/rslp/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mraw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.decode(\u001b[33m\"\u001b[39m\u001b[33mutf8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m     lines = rules.split(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     68\u001b[39m     lines = [line \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines \u001b[38;5;28;01mif\u001b[39;00m line != \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# remove blank lines\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pln/lib/python3.12/site-packages/nltk/data.py:836\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[39m\n\u001b[32m    833\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m>>\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    835\u001b[39m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m836\u001b[39m opened_resource = \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33mraw\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    839\u001b[39m     resource_val = opened_resource.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pln/lib/python3.12/site-packages/nltk/data.py:962\u001b[39m, in \u001b[36m_open\u001b[39m\u001b[34m(resource_url)\u001b[39m\n\u001b[32m    959\u001b[39m protocol, path_ = split_resource_url(resource_url)\n\u001b[32m    961\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol.lower() == \u001b[33m\"\u001b[39m\u001b[33mnltk\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m962\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.open()\n\u001b[32m    963\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m protocol.lower() == \u001b[33m\"\u001b[39m\u001b[33mfile\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    964\u001b[39m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[32m    965\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m]).open()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pln/lib/python3.12/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mrslp\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('rslp')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mstemmers/rslp/step0.pt\u001b[0m\n\n  Searched in:\n    - '/home/penido/nltk_data'\n    - '/home/penido/miniconda3/envs/pln/nltk_data'\n    - '/home/penido/miniconda3/envs/pln/share/nltk_data'\n    - '/home/penido/miniconda3/envs/pln/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "import emoji\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from typing import List\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords_pt = set(stopwords.words('portuguese'))\n",
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "178a48ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ABBREVIATIONS = {\n",
    "    \"vc\": \"você\", \"td\": \"tudo\", \"pq\": \"porque\", \"blz\": \"beleza\",\n",
    "    \"q\": \"que\", \"n\": \"não\", \"ñ\":\"não\", \"ta\": \"tá\", \"eh\": \"é\", \"aki\": \"aqui\",\n",
    "    \"msg\": \"mensagem\", \"tbm\": \"também\", \"hj\": \"hoje\", \"obg\": \"obrigado\", \"tmj\": \"estamos_juntos\", \"fpd\": \"filho_da_puta\"\n",
    "}\n",
    "\n",
    "CONTRACTIONS = {\n",
    "    \"tá\": \"está\", \"tô\": \"estou\", \"cê\": \"você\", \"num\": \"em um\", \"dum\": \"de um\"\n",
    "}\n",
    "\n",
    "SWEAR_WORDS = {\n",
    "    \"foda\", \"merda\", \"caralho\", \"puta\", \"porra\", \"viado\", \"bicha\", \"desgraçado\", \"escroto\"\n",
    "}\n",
    "\n",
    "MWES = [\n",
    "    # Insultos diretos e compostos\n",
    "    \"filho da mãe\",\n",
    "    \"filha da mãe\",\n",
    "    \"filho da puta\",\n",
    "    \"filha da puta\",\n",
    "    \"vai se ferrar\",\n",
    "    \"tomar no cu\",\n",
    "    \"vai tomar no cu\",\n",
    "    \"encher o saco\",\n",
    "    \"encher a porra do saco\",\n",
    "    \"enfia no cu\",\n",
    "    \"vai pra puta que pariu\",\n",
    "    \"mala sem alça\",\n",
    "    \"pau no cu\",\n",
    "    \"boca de bosta\",\n",
    "    \"idiota do caralho\",\n",
    "    \"retardado mental\",\n",
    "    \"bando de lixo\",\n",
    "    \"cabeça de vento\",\n",
    "    \"mente pequena\",\n",
    "    \"mau caráter\",\n",
    "    \"falso moralista\",\n",
    "]\n",
    "\n",
    "def expand_abbreviations(text):\n",
    "    return ' '.join([ABBREVIATIONS.get(w, w) for w in text.split()])\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return ' '.join([CONTRACTIONS.get(w, w) for w in text.split()])\n",
    "\n",
    "def normalize_laughter(text):\n",
    "    return re.sub(r\"(k|rs|ha|he){3,}\", \"risada\", text)\n",
    "\n",
    "def remove_accents(text):\n",
    "    # Substitui apenas acentos, mantendo caracteres especiais como ç e pontuação\n",
    "    normalized = unicodedata.normalize('NFD', text)\n",
    "    # Remove apenas marcas de acento (não remove letras como ç, ñ, etc.)\n",
    "    return ''.join([c for c in normalized if unicodedata.category(c) != 'Mn'])\n",
    "\n",
    "def handle_negation(tokens):\n",
    "    result = []\n",
    "    negate = False\n",
    "    for token in tokens:\n",
    "        if token in {\"não\", \"nunca\", \"jamais\"}:\n",
    "            negate = True\n",
    "            result.append(token.text)\n",
    "        elif token.is_punct or token.is_space:\n",
    "            negate = False\n",
    "            result.append(token.text)\n",
    "        elif negate:\n",
    "            result.append(\"não_\" + token.lemma_)\n",
    "        else:\n",
    "            result.append(token.lemma_)\n",
    "    return result\n",
    "\n",
    "def merge_mwes(text: str, mwe_list: list) -> str:\n",
    "    \"\"\"\n",
    "    Substitui expressões multipalavras por formas unificadas com underscore.\n",
    "    Exemplo: \"filho da mãe\" -> \"filho_da_mãe\"\n",
    "    \"\"\"\n",
    "    for expr in sorted(mwe_list, key=len, reverse=True):\n",
    "        # Cria regex segura, insensível a maiúsculas\n",
    "        pattern = re.compile(r'\\b' + re.escape(expr) + r'\\b', flags=re.IGNORECASE)\n",
    "        replacement = expr.replace(\" \", \"_\")\n",
    "        text = pattern.sub(replacement, text)\n",
    "    return text\n",
    "\n",
    "def replace_named_entities(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Substitui entidades nomeadas por categorias genéricas (e.g. PESSOA, ORG).\n",
    "    Ex: \"Bolsonaro é um lixo\" -> \"ENTIDADE_PESSOA é um lixo\"\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    new_text = text\n",
    "    offset = 0\n",
    "\n",
    "    # Mapeamento personalizado\n",
    "    category_map = {\n",
    "        \"PER\": \"ENTIDADE_PESSOA\",\n",
    "        \"ORG\": \"ENTIDADE_ORGANIZACAO\",\n",
    "        \"LOC\": \"ENTIDADE_LOCAL\",\n",
    "        \"MISC\": \"ENTIDADE_MISC\"\n",
    "    }\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        label = ent.label_\n",
    "        if label in category_map:\n",
    "            start = ent.start_char + offset\n",
    "            end = ent.end_char + offset\n",
    "            replacement = category_map[label]\n",
    "            new_text = new_text[:start] + replacement + new_text[end:]\n",
    "            # Atualiza o offset caso o replacement tenha tamanho diferente\n",
    "            offset += len(replacement) - (end - start)\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "343e313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str, config) -> str:\n",
    "    if config.get(\"lowercase\"):\n",
    "        text = text.lower()\n",
    "\n",
    "    if config.get(\"remove_urls\"):\n",
    "        text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "\n",
    "    if config.get(\"remove_mentions_hashtags\"):\n",
    "        if config.get(\"split_hashtags\"):\n",
    "            text = re.sub(r\"#\", \" \", text)\n",
    "        else:\n",
    "            text = re.sub(r\"@\\w+|#\\w+\", \"\", text)\n",
    "\n",
    "    if config.get(\"remove_emojis\"):\n",
    "        text = emoji.replace_emoji(text, replace='')\n",
    "\n",
    "    if config.get(\"remove_numbers\"):\n",
    "        text = re.sub(r\"\\d+\", \"\", text)\n",
    "    \n",
    "    if config.get(\"replace_named_entities\"):\n",
    "        text = replace_named_entities(text)\n",
    "\n",
    "    if config.get(\"merge_mwes\"):\n",
    "        text = merge_mwes(text, MWES)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = []\n",
    "\n",
    "    negation = False  # controle de escopo de negação\n",
    "\n",
    "    for token in doc:\n",
    "        word = token.text.lower()\n",
    "\n",
    "        if config.get(\"remove_punctuation\") and token.is_punct:\n",
    "            continue\n",
    "\n",
    "        if config.get(\"remove_stopwords\") and word in stopwords_pt:\n",
    "            continue\n",
    "\n",
    "        if config.get(\"min_token_length\") and len(word) < config[\"min_token_length\"]:\n",
    "            continue\n",
    "\n",
    "        if config.get(\"pos_filter\") and token.pos_ not in {\"NOUN\", \"ADJ\"}:\n",
    "            continue\n",
    "\n",
    "        if config.get(\"expand_abbreviations\") and word in ABBREVIATIONS:\n",
    "            word = ABBREVIATIONS[word]\n",
    "        if config.get(\"expand_contractions\") and word in CONTRACTIONS:\n",
    "            word = CONTRACTIONS[word]\n",
    "\n",
    "        if config.get(\"normalize_laughter\"):\n",
    "            word = normalize_laughter(word)\n",
    "\n",
    "        if config.get(\"replace_swears\") and word in SWEAR_WORDS:\n",
    "            filtered_tokens.append(\"palavra_ofensiva\")\n",
    "            continue\n",
    "\n",
    "        # Negation scope handling\n",
    "        if config.get(\"negation_scope\"):\n",
    "            if word in {\"não\", \"nunca\", \"jamais\", \"nem\"}:\n",
    "                negation = True\n",
    "                continue  # opcional: podemos omitir o marcador\n",
    "            elif token.is_punct:\n",
    "                negation = False\n",
    "\n",
    "            if negation:\n",
    "                word = \"NEG_\" + word\n",
    "\n",
    "        # Lematização\n",
    "        if config.get(\"lemmatize\"):\n",
    "            lemma = token.lemma_.strip()\n",
    "            word = lemma if lemma else word\n",
    "\n",
    "        if config.get(\"remove_accents\"):\n",
    "            word = remove_accents(word)\n",
    "\n",
    "        filtered_tokens.append(word)\n",
    "\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2343def",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lowercase\": True,\n",
    "    \"remove_accents\": True,\n",
    "    \"remove_punctuation\": True,\n",
    "    \"remove_numbers\": True,\n",
    "    \"remove_urls\": True,\n",
    "    \"remove_mentions_hashtags\": True,\n",
    "    \"expand_abbreviations\": True,\n",
    "    \"expand_contractions\": False,\n",
    "    \"normalize_laughter\": True,\n",
    "    \"remove_emojis\": True,\n",
    "    \"remove_stopwords\": True,\n",
    "    \"lemmatize\": True,\n",
    "    \"pos_filter\": False,\n",
    "    \"min_token_length\": 2,\n",
    "    \"negation_scope\": False,\n",
    "    \"replace_swears\": False,\n",
    "    \"split_hashtags\": False,\n",
    "    \"merge_mwes\" : True,\n",
    "    \"replace_named_entities\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "acd8080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto =  '[2/8 1:24 pm] bolsonaro terezinha: carol n茫o 茅 filha da puta processo\\n[2/8 1:24 pm] terezinha: 茅 cadeira por dois anos\\n[2/8 1:25 pm] terezinha: sem fian莽a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e34288c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pm bolsonaro terezinhar carol n茫o filha_da_puta processo pm terezinhar cadeira dois ano pm terezinhar fian莽a'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(texto, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273ffab2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
