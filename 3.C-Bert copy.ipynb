{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8db2e44",
   "metadata": {},
   "source": [
    "# Classifica√ß√£o de Discurso de √ìdio \n",
    "-------------------------------------\n",
    "## Abordagem: Bert com pos_weight na BCEWithLogitsLoss\n",
    "\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c756740c",
   "metadata": {},
   "source": [
    "## Importa√ß√µes e Defini√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b26be95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/penido/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from utils import preprocess_text, format_lime_output, print_multilabel_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c09f0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['aggressive', 'hate', 'ageism', 'aporophobia', 'body_shame', 'capacitism', 'lgbtphobia', 'political', 'racism', 'religious_intolerance', 'misogyny', 'xenophobia', 'other']\n",
    "features = 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f762e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√µes para o pr√©-processamento\n",
    "config = {\n",
    "    \"lowercase\": True,  # se BERT uncased\n",
    "    \"remove_accents\": False,\n",
    "    \"remove_punctuation\": False,\n",
    "    \"remove_numbers\": False,\n",
    "    \"remove_urls\": True,\n",
    "    \"remove_mentions_hashtags\": True,\n",
    "    \"expand_abbreviations\": True,\n",
    "    \"expand_contractions\": False,\n",
    "    \"normalize_laughter\": True,\n",
    "    \"remove_emojis\": False,  # ou substitua por <EMOJI>\n",
    "    \"remove_stopwords\": False,\n",
    "    \"lemmatize\": False,\n",
    "    \"stemming\": False,\n",
    "    \"pos_filter\": False,\n",
    "    \"min_token_length\": 2,\n",
    "    \"negation_scope\": False,\n",
    "    \"replace_swears\": False,  # manter palavr√µes √© √∫til para detec√ß√£o de discurso de √≥dio\n",
    "    \"split_hashtags\": True,\n",
    "    \"merge_mwes\": True,  # s√≥ se o seu dicion√°rio for bom\n",
    "    \"replace_named_entities\": False\n",
    "}\n",
    "\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdd9563",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "## Prepara o Conjunto de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7210f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample_majority_class(df, label_col='label', majority_class=0, desired_ratio=0.3, random_state=42):\n",
    "    # Separa a classe majorit√°ria (classe 0)\n",
    "    df_majority = df[df[label_col] == majority_class]\n",
    "\n",
    "    # Separa todas as outras classes\n",
    "    df_others = df[df[label_col] != majority_class]\n",
    "\n",
    "    # N√∫mero total de amostras desejado ap√≥s o balanceamento\n",
    "    total_desired = len(df_others) / (1 - desired_ratio)\n",
    "\n",
    "    # Quantidade desejada de amostras da classe majorit√°ria\n",
    "    n_majority_desired = int(total_desired * desired_ratio)\n",
    "\n",
    "    # Undersample da classe majorit√°ria\n",
    "    df_majority_downsampled = df_majority.sample(n=n_majority_desired, random_state=random_state)\n",
    "\n",
    "    # Junta os dados\n",
    "    df_balanced = pd.concat([df_majority_downsampled, df_others])\n",
    "\n",
    "    # Embaralha (opcional mas recomendado)\n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    return df_balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b747c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Carrega o dataset TuPyE multilabel\n",
    "df = load_dataset(\"Silly-Machine/TuPyE-Dataset\", name=\"multilabel\")\n",
    "\n",
    "train_df = df['train'].to_pandas()\n",
    "train_df[\"label_comb\"] = train_df[target].astype(str).agg(\"\".join, axis=1)\n",
    "train_df[\"class_id\"] = le.fit_transform(train_df[\"label_comb\"])\n",
    "\n",
    "train_df = undersample_majority_class(train_df, label_col='class_id', majority_class=0, desired_ratio=0.5)\n",
    "\n",
    "test_df = df['test'].to_pandas()\n",
    "\n",
    "X_train_raw = train_df[features]\n",
    "y_train = train_df[target].values\n",
    "\n",
    "X_test_raw = test_df[features]\n",
    "y_test = test_df[target].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848d467d",
   "metadata": {},
   "source": [
    "### Aplica Pr√©-Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05ccd417",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_raw.apply(lambda x: preprocess_text(x, config))\n",
    "X_test = X_test_raw.apply(lambda x: preprocess_text(x, config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d04f39",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "## Treinamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01bfbb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Tokenizador BERT em portugu√™s\n",
    "model_name = \"neuralmind/bert-base-portuguese-cased\"\n",
    "# model_name= \"neuralmind/bert-large-portuguese-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37c3a104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Crie seu dataset personalizado\n",
    "class MultilabelDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf1e98b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MultilabelDataset(X_train.tolist(), y_train, tokenizer)\n",
    "test_dataset = MultilabelDataset(X_test.tolist(), y_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf383079",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da5c6f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor = torch.tensor(y_train)  # ou use diretamente o tensor\n",
    "\n",
    "# Contagem de positivos e negativos por classe\n",
    "pos_counts = y_train_tensor.sum(dim=0)\n",
    "neg_counts = y_train_tensor.shape[0] - pos_counts\n",
    "\n",
    "# Peso positivo = (n negativos) / (n positivos)\n",
    "pos_weight = neg_counts / (pos_counts + 1e-5)  # evita divis√£o por zero\n",
    "pos_weight = torch.min(pos_weight, torch.tensor(10.0))\n",
    "pos_weight = pos_weight.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ba538e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2607,  3.8377, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000, 10.0000,\n",
       "        10.0000, 10.0000, 10.0000, 10.0000,  4.6498], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c73bec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultilabelTrainer(Trainer):\n",
    "    def __init__(self, *args, pos_weight=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.pos_weight = pos_weight\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight)\n",
    "\n",
    "    def compute_loss(self, model, inputs,num_items_in_batch, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = self.loss_fn(logits, labels.float())  # BCE precisa de float\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd1bdc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(29794, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=13, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(target), problem_type=\"multi_label_classification\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1f6776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',       # Ainda obrigat√≥rio, mas o Trainer n√£o vai salvar nada\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=4,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"no\",           # <-- desativa o salvamento de modelos/checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b41ca13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "# )\n",
    "\n",
    "trainer = MultilabelTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    pos_weight=pos_weight \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2eb7af09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8132' max='8132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8132/8132 1:18:15, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.650500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.555800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.583900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.485300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.486300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.472600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.418300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.435800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.435100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.379300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.428300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.421900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.434800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.407400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.396200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.404700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.382700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.389400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.385400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.431400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.349000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.407000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.367400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.383500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.359100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.362300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.357100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.364800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.351300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.375900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.308100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.359900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.337800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.358900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.302000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.379200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.368600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.330400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.331800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.298800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.261300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.330700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.291800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.263000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.298800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.295200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.286900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.317800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.276600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.275200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.282000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.250700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.207700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.300800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.280900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.314100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.258600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.306200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.304400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.262900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.262600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.288900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.286000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.262100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.239900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.264300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.265400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.309200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.285200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.235100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.254900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.245400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.275500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.297800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.246100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.240700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.226900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.256200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.288200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.229400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.197100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.207900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.178200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.157500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.188400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.188000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.183100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.155600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.170300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.161500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.253500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.185900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.171000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.147800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.170600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.166400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.197600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.190600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.148500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.175800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.198100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.167000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.197000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.214000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.166600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.163200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.185400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.190900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>0.192700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.170900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>0.145900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.137100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.200300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.171100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>0.223000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.174400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>0.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.248200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>0.177600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.202200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>0.126600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.109200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.105800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.127600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>0.141700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.109400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>0.142600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.106500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>0.120300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.107700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>0.138900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.120200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>0.145500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.128500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>0.133000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.125800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>0.109200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.128200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>0.102400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.114100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>0.102800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>0.112400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.156300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>0.091200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.108000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>0.097900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.113100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>0.127800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.114500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>0.094600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.137700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>0.104300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.107400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>0.128400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.098600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>0.094200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.137500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>0.124300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.132800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8132, training_loss=0.2450932721111262, metrics={'train_runtime': 4695.6655, 'train_samples_per_second': 17.316, 'train_steps_per_second': 1.732, 'total_flos': 5349049777741824.0, 'train_loss': 0.2450932721111262, 'epoch': 4.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39daea6e",
   "metadata": {},
   "source": [
    "-----------------------------------\n",
    "## Avalia√ß√£o no conjunto de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee69369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    model.eval()  # Garante que o modelo est√° em modo de avalia√ß√£o\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokeniza√ß√£o e envio dos inputs para o mesmo device do modelo\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.sigmoid(outputs.logits).cpu().numpy()[0]  # move o resultado de volta para CPU para convers√£o\n",
    "\n",
    "    return dict(zip(target, probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "048b1900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aggressive': 0.990462, 'hate': 0.03183589, 'ageism': 0.0008927432, 'aporophobia': 0.0016496172, 'body_shame': 0.0012331516, 'capacitism': 0.0031170864, 'lgbtphobia': 0.002713505, 'political': 0.008915195, 'racism': 0.002239556, 'religious_intolerance': 0.0026345241, 'misogyny': 0.0034058397, 'xenophobia': 0.0015203615, 'other': 0.04311796}\n"
     ]
    }
   ],
   "source": [
    "# Exemplo:\n",
    "print(predict(\"Esse cara √© um idiota in√∫til\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00e171bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aggressive': 0.008286432, 'hate': 0.0030249502, 'ageism': 0.0013711029, 'aporophobia': 0.00087756943, 'body_shame': 0.002387941, 'capacitism': 0.0009954101, 'lgbtphobia': 0.0029321609, 'political': 0.0026831757, 'racism': 0.004378396, 'religious_intolerance': 0.0014177023, 'misogyny': 0.003859935, 'xenophobia': 0.004413594, 'other': 0.011459826}\n"
     ]
    }
   ],
   "source": [
    "# Exemplo:\n",
    "print(predict(\"bom dia\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12ccf768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(29794, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=13, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b951383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "all_probs = []\n",
    "all_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ac79bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 546/546 [03:25<00:00,  2.66it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 2. Infer√™ncia por batches no test set\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "        labels = batch[\"labels\"].cpu().numpy()\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "\n",
    "        all_probs.append(probs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "# 3. Concatenar tudo\n",
    "all_probs = np.concatenate(all_probs, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36a1a223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_probs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "667d3fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (all_probs >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "374a100b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Avalia√ß√£o Multilabel\n",
      "==============================\n",
      "‚úîÔ∏è F1 Score (Micro):     0.6266\n",
      "‚úîÔ∏è F1 Score (Macro):     0.4656\n",
      "‚úîÔ∏è F1 Score (Weighted):  0.6308\n",
      "‚ö†Ô∏è Hamming Loss:         0.0402\n",
      "‚úÖ Subset Accuracy:      0.7005\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_multilabel_metrics(all_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b31840e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Avalia√ß√£o Multilabel\n",
      "==============================\n",
      "‚úîÔ∏è F1 Score (Micro):     0.6718\n",
      "‚úîÔ∏è F1 Score (Macro):     0.6585\n",
      "‚úîÔ∏è F1 Score (Weighted):  0.6719\n",
      "‚ö†Ô∏è Hamming Loss:         0.1419\n",
      "‚úÖ Subset Accuracy:      0.7845\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# Considerando s√≥ agressive e hate\n",
    "print_multilabel_metrics(all_labels[:, :2], preds[:, :2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a7afcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def otimizar_thresholds_weighted(y_true, y_pred_proba, thresholds=np.arange(0.0, 1.01, 0.01)):\n",
    "    \"\"\"\n",
    "    Otimiza o threshold para cada label individualmente maximizando o F1-score (weighted).\n",
    "\n",
    "    Par√¢metros:\n",
    "        y_true: ndarray bin√°rio de shape (n_amostras, n_labels)\n",
    "        y_pred_proba: ndarray de probabilidades (entre 0 e 1), mesma shape\n",
    "        thresholds: array de thresholds a serem testados\n",
    "\n",
    "    Retorna:\n",
    "        thresholds_otimos: lista com melhor threshold para cada label\n",
    "        f1_scores: lista com F1-score (weighted) correspondente ao melhor threshold\n",
    "    \"\"\"\n",
    "    n_labels = y_true.shape[1]\n",
    "    thresholds_otimos = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for i in tqdm(range(n_labels), desc=\"Otimizando thresholds (F1 weighted)\"):\n",
    "        f1_max = -1\n",
    "        best_thresh = 0.5\n",
    "        for t in thresholds:\n",
    "            y_pred_bin = (y_pred_proba[:, i] >= t).astype(int)\n",
    "            f1 = f1_score(y_true[:, i], y_pred_bin, average='weighted', zero_division=0)\n",
    "            if f1 > f1_max:\n",
    "                f1_max = f1\n",
    "                best_thresh = t\n",
    "        thresholds_otimos.append(best_thresh)\n",
    "        f1_scores.append(f1_max)\n",
    "\n",
    "    return thresholds_otimos, f1_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abae0719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def otimizar_thresholds_acuracia(y_true, y_pred_proba, thresholds=np.arange(0.0, 1.01, 0.01)):\n",
    "    \"\"\"\n",
    "    Encontra o melhor threshold para cada label maximizando a acur√°cia.\n",
    "\n",
    "    Par√¢metros:\n",
    "        y_true: ndarray bin√°rio de shape (n_amostras, n_labels)\n",
    "        y_pred_proba: ndarray de probabilidades (n_amostras, n_labels)\n",
    "        thresholds: array de valores de corte a testar\n",
    "\n",
    "    Retorna:\n",
    "        thresholds_otimos: lista com melhor threshold para cada label\n",
    "        acc_scores: lista com acur√°cia correspondente a cada threshold √≥timo\n",
    "    \"\"\"\n",
    "    n_labels = y_true.shape[1]\n",
    "    thresholds_otimos = []\n",
    "    acc_scores = []\n",
    "\n",
    "    for i in tqdm(range(n_labels), desc=\"Otimizando thresholds (Acur√°cia)\"):\n",
    "        acc_max = -1\n",
    "        best_thresh = 0.5\n",
    "        for t in thresholds:\n",
    "            y_pred_bin = (y_pred_proba[:, i] >= t).astype(int)\n",
    "            acc = accuracy_score(y_true[:, i], y_pred_bin)\n",
    "            if acc > acc_max:\n",
    "                acc_max = acc\n",
    "                best_thresh = t\n",
    "        thresholds_otimos.append(best_thresh)\n",
    "        acc_scores.append(acc_max)\n",
    "\n",
    "    return thresholds_otimos, acc_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25d417fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Otimizando thresholds (Acur√°cia): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:08<00:00,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: Threshold √≥timo = 0.82, F1 weighted = 0.8505\n",
      "Label 1: Threshold √≥timo = 0.90, F1 weighted = 0.9132\n",
      "Label 2: Threshold √≥timo = 0.93, F1 weighted = 0.9986\n",
      "Label 3: Threshold √≥timo = 0.70, F1 weighted = 0.9987\n",
      "Label 4: Threshold √≥timo = 0.93, F1 weighted = 0.9956\n",
      "Label 5: Threshold √≥timo = 0.91, F1 weighted = 0.9987\n",
      "Label 6: Threshold √≥timo = 0.81, F1 weighted = 0.9916\n",
      "Label 7: Threshold √≥timo = 0.95, F1 weighted = 0.9807\n",
      "Label 8: Threshold √≥timo = 1.00, F1 weighted = 0.9936\n",
      "Label 9: Threshold √≥timo = 0.99, F1 weighted = 0.9981\n",
      "Label 10: Threshold √≥timo = 0.98, F1 weighted = 0.9745\n",
      "Label 11: Threshold √≥timo = 0.99, F1 weighted = 0.9904\n",
      "Label 12: Threshold √≥timo = 0.97, F1 weighted = 0.9171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "thresholds_otimos, f1s = otimizar_thresholds_acuracia(all_labels, all_probs)\n",
    "\n",
    "for i, (t, f) in enumerate(zip(thresholds_otimos, f1s)):\n",
    "    print(f\"Label {i}: Threshold √≥timo = {t:.2f}, F1 weighted = {f:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a503b964",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.zeros_like(all_probs, dtype=int)\n",
    "\n",
    "for i, threshold in enumerate(thresholds_otimos):\n",
    "    preds[:, i] = (all_probs[:, i] >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4aee68d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Avalia√ß√£o Multilabel\n",
      "==============================\n",
      "‚úîÔ∏è F1 Score (Micro):     0.6213\n",
      "‚úîÔ∏è F1 Score (Macro):     0.3875\n",
      "‚úîÔ∏è F1 Score (Weighted):  0.6082\n",
      "‚ö†Ô∏è Hamming Loss:         0.0307\n",
      "‚úÖ Subset Accuracy:      0.7660\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "print_multilabel_metrics(all_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7196391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
